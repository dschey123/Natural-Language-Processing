{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8bc1df-8400-48c4-8389-a76763b75fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import fuzz,process\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#berttopic, \n",
    "from transformers import pipeline\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe3a25-56f9-4950-88a6-46736197888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glob Files\n",
    " \n",
    "# list all csv files only\n",
    "csv_files = glob.glob('./Raw Files/*.{}'.format('csv'))\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf02af-917c-48eb-9713-4459ad6c3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./Raw Files/raw_survey_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b47e87-2361-4269-ad16-21e34b183ec0",
   "metadata": {},
   "source": [
    "# Initial Preprocessing code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd83d9cd-b8e3-4875-a448-b22537bdd909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove shorter, less informative survey responses - apply to remove shorter responses\n",
    "short_responses_df=df[df['raw_translation'].str.len()<7]\n",
    "medium_responses_df=df[(df['raw_translation'].str.len()>=7) & (df['raw_translation'].str.len()<=35)]\n",
    "df=df[df['raw_translation'].str.len()>35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c9ee0-7011-41b1-a8bc-7274805de378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' column to datetime format \n",
    "df['date'] = pd.to_datetime(df['Day of Survey Date'].str.replace(r'\\b(\\d{1,2})(st|nd|rd|th)\\b', r'\\1', regex=True), format='%B %d, %Y')\n",
    "# Extract month and year\n",
    "df['month_year'] = df['date'].dt.to_period('M') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e1c48e-c5ef-4b1d-b052-416da8283674",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month_year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26729621-a79c-4c0c-ac82-c7957323f6ec",
   "metadata": {},
   "source": [
    "# Preprocessing Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814663d6-3eef-4708-80b9-ff15b0d2c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import BertTokenizer\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "#add as needed depending on context\n",
    "contractions_dict={\n",
    "    'dr':'doctor',\n",
    "    'dr.':'doctor',\n",
    "    'doc':'doctor',\n",
    "    'doc.':'doctor',\n",
    "    'prov':'provider',\n",
    "    'prov.':'provider',\n",
    "    'mr':'mister',\n",
    "    'mr.':'mister',\n",
    "    'mrs.':'misses',\n",
    "    'mrs':'misses',\n",
    "    'rep':'representative',\n",
    "    'rep.':'representative',\n",
    "     \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"who's\": \"who is\", \n",
    "    \"what's\": \"what is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"n't\": \" not\",\n",
    "    \"'re\": \" are\",\n",
    "    \"'s\": \" is\",\n",
    "    \"'d\": \" would\",\n",
    "    \"'ll\": \" will\",\n",
    "    \"'ve\": \" have\",\n",
    "    \"'m\": \" am\" }\n",
    "\n",
    "#useful for non-bert models\n",
    "def in_depth_preprocess_text(text,contractions_dict,lowercase=True):\n",
    "\n",
    "    #can omit if using cased model    \n",
    "    if lowercase: #lowercasing\n",
    "        text=text.lower()\n",
    "    \n",
    "    for contraction, expanded in contractions_dict.items(): #handle contractions\n",
    "        text=re.sub(r'\\b' + contraction + r'\\b',expanded,text,flags=re.IGNORECASE)\n",
    "    \n",
    "    text=re.sub(r'(.)\\1{2,}',r'\\1',text) #fix common character repetitions (eg. iiii -> i)\n",
    "    text=re.sub(r'[^\\w\\s]', '',text) #remove punctuation\n",
    "    text=re.sub(r'\\s+', ' ',text).strip() #normalize whitespace\n",
    "    \n",
    "    words=text.split()\n",
    "    #unique_words=set(word for word in words if re.match(r'^[a-zA-Z]+$',word)) #remove duplicate words\n",
    "    cleaned_text=' '.join(words)\n",
    "        \n",
    "    doc=nlp(cleaned_text) #additional preprocessing & lemmatization\n",
    "    lemmatized_tokens=[token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    cleaned_text= ' '.join(lemmatized_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "#use for trained models like Bert/Transformers\n",
    "def simplified_preprocess_text(text,contractions_dict,lowercase=True):\n",
    "    \n",
    "    #can omit if using cased model\n",
    "    if lowercase: #lowercasing\n",
    "        text=text.lower()\n",
    "        \n",
    "    for contraction, expanded in contractions_dict.items(): #handle contractions\n",
    "        text=re.sub(r'\\b' + contraction + r'\\b',expanded,text,flags=re.IGNORECASE)\n",
    "    \n",
    "    text=re.sub(r'(.)\\1{2,}',r'\\1',text) #fix common character repetitions (eg. iiii -> i)\n",
    "    text=re.sub(r'\\s+', ' ',text).strip() #normalize whitespace\n",
    "    \n",
    "    #tokenizer=BertTokenizer.from_pretrained('bert-base-uncased') #bert tokenizer\n",
    "    #tokens=tokenizer.tokenize(cleaned_text)\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37be40-d729-4af4-a046-24d5abfe53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different preprocessing is better for different cased/uncased models\n",
    "df['simplified_preprocess_text_lower']=df['raw_translation'].apply(lambda x:simplified_preprocess_text(x,contractions_dict))\n",
    "df['simplified_preprocess_text_orig']=df['raw_translation'].apply(lambda x:simplified_preprocess_text(x,contractions_dict,lowercase=False))\n",
    "df['in_depth_preprocess_text']=df['raw_translation'].apply(lambda x:in_depth_preprocess_text(x,contractions_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456098ef-e8c6-4bf8-b214-8aee58a6e681",
   "metadata": {},
   "source": [
    "# EDA - Extra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a0e07-34ab-4490-bae0-ffafb6ee3e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['raw_character_count']=df['Open Ended Response'].apply(len)\n",
    "df['raw_word_count']=df['Open Ended Response'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec2ebc-45a9-4c0d-9a66-8ac7c793774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#readability score\n",
    "import textstat\n",
    "\n",
    "df['readiability_score']=df['raw_translation'].apply(textstat.flesch_kincaid_grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1d3fd9-ac7b-41b0-a04f-52d9b4fb7f7d",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e07f39-ff84-4871-8782-64e413c20243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb24f6-6101-4768-82e9-065f996f988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_analyzer=pipeline('sentiment-analysis',model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "roberta_analyzer=pipeline('sentiment-analysis',model='cardiffnlp/twitter-roberta-base-sentiment')\n",
    "distilbert_analyzer=pipeline('sentiment-analysis',model='distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6b378-6d9a-448c-acf9-e11d8c500448",
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_analyzer=TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b73255d-fe15-4a91-ba16-8ca266bae370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sentiment_analysis(row):\n",
    "    text_processed_orig=row['simplified_preprocess_text_orig']\n",
    "    text_processed_new=row['simplified_preprocess_text_lower']\n",
    "    \n",
    "    bert_result=bert_analyzer(text_processed_new[:512])[0]\n",
    "    distilbert_result=distilbert_analyzer(text_processed_new[:512])[0]\n",
    "    sentence=Sentence(text_processed_new[:512])\n",
    "    flair_analyzer.predict(sentence)\n",
    "    flair_result=sentence.labels[0]\n",
    "    \n",
    "    roberta_result=roberta_analyzer(text_processed_orig[:512])[0]\n",
    "    \n",
    "    \n",
    "    #sentiment_results={\n",
    "    #    'bert':{'label':bert_result['label'],'score':bert_result['score']},\n",
    "    #    'distilbert':{'label':distilbert_result['label'],'score':distilbert_result['score']},\n",
    "    #    'flair':{'label':flair_result.value,'score':flair_result.score},\n",
    "    #    'roberta':{'label':roberta_result['label'],'score':roberta_result['score']}\n",
    "    #}\n",
    "    \n",
    "    return pd.Series({\n",
    "        'bert_label':bert_result['label'],\n",
    "        'bert_score':bert_result['score'],\n",
    "        'distilbert_label':distilbert_result['label'],\n",
    "        'distilbert_score':distilbert_result['score'],        \n",
    "        'flair_label':flair_result.value,\n",
    "        'flair_score':flair_result.score,\n",
    "        'roberta_label':roberta_result['label'],\n",
    "        'roberta_score':roberta_result['score']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e037f9-c53d-478f-8a42-a56b92c09b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['bert_label','bert_score',\n",
    "    'distilbert_label','distilbert_score',\n",
    "    'flair_label','flair_score',\n",
    "    'roberta_label','roberta_score']]=df.apply(apply_sentiment_analysis,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687df2a3-21ec-4d74-bac1-96f4e9f80a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs of sentiment models need to be mapped to \"Positive\",\"Negative\" and \"Neutral\" labels\n",
    "def map_labels_to_sentiment(label,column_name):\n",
    "    if column_name=='bert_label':\n",
    "        if label in ['5 stars','4 stars']:\n",
    "            return 'POSITIVE'\n",
    "        elif label in ['1 stars', '2 stars']:\n",
    "            return 'NEGATIVE'\n",
    "        elif label == '3 stars':\n",
    "            return 'NEUTRAL'\n",
    "\n",
    "    elif column_name=='distilbert_label':\n",
    "        if label == 'POSITIVE':\n",
    "            return 'POSITIVE'\n",
    "        elif label == 'NEGATIVE':\n",
    "            return 'NEGATIVE'\n",
    "\n",
    "    elif column_name=='roberta_label':\n",
    "        if label == 'LABEL_2':\n",
    "            return 'POSITIVE'\n",
    "        elif label == 'LABEL_0':\n",
    "            return 'NEGATIVE'   \n",
    "        elif label == 'LABEL 1':\n",
    "            return 'NEUTRAL'\n",
    "        \n",
    "    elif column_name=='flair_label':\n",
    "        if label == 'POSITIVE':\n",
    "            return 'POSITIVE'\n",
    "        elif label == 'NEGATIVE':\n",
    "            return 'NEGATIVE'   \n",
    "\n",
    "    return 'NEUTRAL'        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad43b5e-1e93-473b-bebf-b74bd20ac7aa",
   "metadata": {},
   "outputs": [],
   "source": [
   "# 4-pronged Sentiment Analysis approach for Accuracy - 3 or more of the models have to agree on outputs \n",
    "def aggregate_sentiments(row):\n",
    "    \n",
    "    sentiment_columns=['bert_label','distilbert_label','roberta_label','flair_label']\n",
    "    sentiment_counts={'POSITIVE':0,'NEGATIVE':0,'NEUTRAL':0}\n",
    "\n",
    "    for column in sentiment_columns:\n",
    "        sentiment=map_labels_to_sentiment(row[column],column)\n",
    "        sentiment_counts[sentiment]+=1\n",
    "    \n",
    "    if sentiment_counts['POSITIVE']>=3:\n",
    "        return 'POSITIVE'\n",
    "    elif sentiment_counts['NEGATIVE']>=3:\n",
    "        return 'NEGATIVE'\n",
    "    else:\n",
    "        return 'NEUTRAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a72ab-0252-40c3-a00e-95a8dcf568ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['final_sentiment']=df.apply(aggregate_sentiments,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465b6ad4-c942-4555-85bd-ea9867186f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent=df.final_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d1d998-37ba-43e3-9c0d-c97a5d302486",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=['green','orange','red']\n",
    "df_sent.plot(kind='bar',color=colors)\n",
    "plt.title('Value Counts for Sentiments Uncovered')\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd107e-784d-439d-be18-f77e12797606",
   "metadata": {},
   "source": [
    "# Identifying Medical Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5701d911-bc54-49bf-bad6-e726af62e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import regex package***\n",
    "# Mapping dictionary \n",
    "medical_areas = { \"cardiology\": [\"heart\", \"cardio\",'cardiologist'],\n",
    "                         \"dermatology\": [\"skin\", \"derma\",'dermatologist'],\n",
    "                         \"oncology\": [\"cancer\", \"tumor\", \"oncologist\"],\n",
    "                         \"dentistry\": [\"teeth\", \"dentist\",'dental','jaw'],\n",
    "                         \"neurology\": [\"nervous\", \"brain\",'neuro'],\n",
    "                         \"pediatrics\": [\"pediatrician\",'pediatrist'],\n",
    "                        \"urology\": ['kidney','urine','urinary','urinate'],\n",
    "                         \"gastroenterology\": [\"stomach\", \"gut\", \"digestive\", \"gastro\",'gastroenterologist'],\n",
    "                         \"orthopedics/podiatry\": [\"bones\", \"joints\",'knee','toe', \"orthopedist\",'ankle','foot','podiatrist'],\n",
    "                         \"gynecology\": [\"gynecologist\",'reproductive','ovaries','vagina','uterus'],\n",
    "                         \"rheumatology\": [\"arthritis\",\"rheumatologist\"],\n",
    "                         \"endocrinology\": [\"hormones\", \"glands\", \"endocrinologist\"],\n",
    "                         \"psychiatry/psychology\": [\"psychiatrist\",'psychologist','therapist','counseling', \"psych\"],\n",
    "                         \"pulmonology\": [\"lungs\", \"respiratory\", \"pulmonary\",'pulmonologist'],\n",
    "                         \"hematology\": [\"hematologist\",'anemia','blood'],\n",
    "                         \"nephrology\": [\"kidneys\", \"renal\", \"nephrologist\"],\n",
    "                         \"ophthalmology\": [\"eyesight\", \"vision\", \"ophthalmologist\"],\n",
    "                         \"otolaryngology\": ['otolaryngologist'],\n",
    "                         \"immunology\": [\"allergies\", \"immunologist\"],\n",
    "                         \"radiology\": [\"x-ray\", \"radiologist\"],\n",
    "                         \"anesthesiology\": [\"anesthesia\",'anesthesiologist', \"anesthetist\"],\n",
    "                         \"physical therapy\": [\"rehabilitation\", \"physiotherapy\",'physical therapy',\"PT\"]\n",
    "                        } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b07bba-1c19-43e5-ba24-e24b00c6da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#refine in V3\n",
    "def extract_medical_areas(text):\n",
    "    text_cleaned = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    matched_areas=set()\n",
    "    \n",
    "    for area,keywords in medical_areas.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in text_cleaned:\n",
    "                matched_areas.add(area)\n",
    "                \n",
    "                continue \n",
    "                \n",
    "            elif fuzz.partial_ratio(keyword,text_cleaned)>95:\n",
    "                matched_areas.add(area)\n",
    "                \n",
    "    return ', '.join(sorted(matched_areas))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171ce64-e00f-4901-822b-91e3bdef96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only on survey responses longer than 3 characters*\n",
    "df['medical_presence']=df['raw_translation'].apply(lambda x: extract_medical_areas(x) if len(x) >= 25 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd0a07-3029-4bd5-8723-e41b0d63a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.medical_presence.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae31671-ce88-49bb-be54-e2b14c63d97d",
   "metadata": {},
   "source": [
    "# Usefulness Score for Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38307c83-1e16-47b1-a383-6ea5dbc63106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can apply more to preprocessed text or raw text (v2)\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.util import ngrams \n",
    "from nltk.corpus import stopwords, words \n",
    "import pandas as pd\n",
    "import re \n",
    "import nltk\n",
    "\n",
    "nltk.download('words')\n",
    "\n",
    "# Define stopwords and command words \n",
    "#stop_words = set(stopwords.words('english'))\n",
    "command_words = set([ 'need', 'must', 'should', 'stop', 'fix', 'do', 'don’t', 'can’t', 'please', 'ensure', 'verify', 'check', 'validate', 'confirm', 'achieve', 'address', 'follow', 'complete', 'review', 'implement', 'execute', 'provide', 'resolve', 'avoid', 'improve', 'assess', 'analyze', 'remove', 'adjust', 'notify', 'update', 'submit', 'request', 'clarify', 'refrain', 'consider', 'begin', 'start', 'facilitate','necessary']) \n",
    "\n",
    "# Load a list of valid words \n",
    "valid_words = set(words.words())\n",
    "\n",
    "# Function to check for non-words \n",
    "def is_non_word(token): \n",
    "    return token.lower() not in valid_words and not re.match(r'^[a-zA-Z]+$', token) \n",
    "\n",
    "# Function to calculate non-word penalty\n",
    "def calculate_non_word_penalty(tokens,valid_words): \n",
    "    non_words = sum(1 for token in tokens if is_non_word(token))\n",
    "    return min(non_words / len(tokens), 0.25) if len(tokens)>0 else 0\n",
    "\n",
    "# Function to calculate command word score \n",
    "def calculate_command_word_score(text, command_words):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    command_word_count = sum(1 for word in tokens if word in command_words)\n",
    "    return command_word_count / len(tokens) if len(tokens) > 0 else 0\n",
    "\n",
    "# Function to calculate n-gram frequencies across the corpus \n",
    "def calculate_corpus_ngram_frequencies(corpus_texts, ngram_range=2): \n",
    "    all_ngrams = [] \n",
    "    for text in corpus_texts:\n",
    "        tokens = word_tokenize(text.lower()) \n",
    "        for n in range(2, ngram_range + 1):\n",
    "            ngrams_list = list(ngrams(tokens, n))\n",
    "            all_ngrams.extend(ngrams_list)\n",
    "    return Counter(all_ngrams) \n",
    "\n",
    "# Function to calculate n-gram repetition score for a single text\n",
    "def calculate_ngram_repetition_score(text, ngram_freqs, ngram_range=2):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    ngram_repetition_penalty = 0 \n",
    "    \n",
    "    for n in range(2, ngram_range + 1):\n",
    "        ngrams_list = list(ngrams(tokens, n))\n",
    "        ngrams_count = Counter(ngrams_list)\n",
    "        total_ngrams = len(ngrams_list) \n",
    "        \n",
    "        max_repetition = 0\n",
    "        for ngram, count in ngrams_count.items():\n",
    "            if ngram in ngram_freqs:\n",
    "                max_repetition = max(max_repetition, count / ngram_freqs[ngram]) \n",
    "        \n",
    "        if total_ngrams > 0:\n",
    "            ngram_repetition_penalty = min(max_repetition, 0.25) \n",
    "            \n",
    "    return ngram_repetition_penalty \n",
    "\n",
    "# Function to calculate usefulness score \n",
    "def calculate_usefulness_score(processed_text_2, raw_translation, sentiment_label, medical_presence, ngram_freqs, ngram_range=2):\n",
    "    tokens_processed = word_tokenize(processed_text_2.lower()) \n",
    "    tokens_raw = word_tokenize(raw_translation.lower())     \n",
    "    length_score = min(len(tokens_processed) / 25, 1)\n",
    "    #non_stop_tokens = [word for word in tokens_processed if word not in stop_words]\n",
    "    #repetition_score = 1 - (len(non_stop_tokens) / len(set(non_stop_tokens)))\n",
    "    non_word_penalty_processed=calculate_non_word_penalty(tokens_processed,valid_words)\n",
    "    non_word_penalty_raw=calculate_non_word_penalty(tokens_raw,valid_words)\n",
    "    non_word_penalty = max(non_word_penalty_processed,non_word_penalty_raw)\n",
    "    \n",
    "    # Command words from both processed and raw text \n",
    "    command_word_score_processed = calculate_command_word_score(processed_text_2, command_words)\n",
    "    command_word_score_raw = calculate_command_word_score(raw_translation, command_words)\n",
    "    command_word_score = max(command_word_score_processed, command_word_score_raw)\n",
    "    \n",
    "    ngram_repetition_penalty = calculate_ngram_repetition_score(processed_text_2, ngram_freqs, ngram_range)\n",
    "    sentiment_weight = 1.1 if sentiment_label in ['POSITIVE', 'NEGATIVE'] else 1.0\n",
    "    medical_weight = 1.1 if pd.notna(medical_presence) and medical_presence.strip() != '' else 1\n",
    "    \n",
    "    final_score = (0.35 * length_score + \n",
    "                  # 0.15 * (1 - repetition_score) +\n",
    "                   0.15 * (1 - non_word_penalty) +\n",
    "                   0.15 * (1 - ngram_repetition_penalty) +\n",
    "                   0.10 * command_word_score +\n",
    "                   0.15 * sentiment_weight +\n",
    "                   0.10 * medical_weight)\n",
    "    return final_score \n",
    "\n",
    "corpus_texts = df['in_depth_preprocess_text'].tolist()\n",
    "ngram_freqs = calculate_corpus_ngram_frequencies(corpus_texts, ngram_range=2) \n",
    "\n",
    "# Then, calculate the usefulness score for each review \n",
    "df['usefulness_score'] = df.apply(lambda row: calculate_usefulness_score(row['in_depth_preprocess_text'], row['simplified_preprocess_text_orig'], row['final_sentiment'], row['medical_presence'], ngram_freqs), axis=1)\n",
    "#print(df[['processed_text_2', 'raw_translation', 'final_sentiment', 'medical_presence', 'usefulness_score']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83becd32-5da3-43c8-ac8c-48c5934c0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('final_sentiment')['usefulness_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9cb8a4-889b-43b2-b585-e531977b43c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usefulness_scores over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c5a56-149d-4f28-bbcf-2c4ed11f6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month_year']=df['month_year'].astype(str)\n",
    "df['usefulness_score']=pd.to_numeric(df['usefulness_score'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f4557-5225-40e8-8bb4-c3f0ca2c491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_usefulness_over_time=df.groupby('month_year')['usefulness_score'].mean().reset_index()\n",
    "avg_usefulness_per_sentiment=df.groupby(['month_year','final_sentiment'])['usefulness_score'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d32b2-0e97-448a-b877-a74a5101fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "color_palette_2 = {'POSITIVE':'Green',\n",
    "                   'NEGATIVE':'Red',\n",
    "                   'NEUTRAL':'Orange'}\n",
    "\n",
    "for sentiment,color in color_palette_2.items():\n",
    "    sentiment_data=avg_usefulness_per_sentiment[avg_usefulness_per_sentiment['final_sentiment']==sentiment]\n",
    "    sns.lineplot(x='month_year',y='usefulness_score',data=sentiment_data,label=sentiment,color=color)\n",
    "\n",
    "sns.lineplot(x='month_year',y='usefulness_score',data=avg_usefulness_over_time,label='Overall Average Usefulness',color='black')\n",
    "    \n",
    "\n",
    "plt.title('Average Usefulness Over Time')\n",
    "plt.xlabel('Month-Year')\n",
    "plt.ylabel('Usefulness Score')\n",
    "\n",
    "plt.legend(title='Sentiment')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
